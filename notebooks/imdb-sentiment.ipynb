{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Set to control training\n",
    "do_train=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Pre-trained spaCy English Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel after installing model and skip this cell\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pre = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors are Fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_pre(\"Queen Man Women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queen_token = doc[0]\n",
    "man_token = doc[1]\n",
    "women_token = doc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queen_token.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery = man_token.vector - women_token.vector + queen_token.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What word is most similar to 'mystery'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy import spatial\n",
    "\n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "\n",
    "similarities = []\n",
    "\n",
    "for word in tqdm(nlp_pre.vocab):\n",
    "    # Ignore words without vectors\n",
    "    if not word.has_vector:\n",
    "        continue\n",
    "    similarity = cosine_similarity(mystery, word.vector)\n",
    "    similarities.append((word, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = sorted(similarities, key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([w[0].text for w in similarities[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Fun Spacy Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_pre(u\"Only months following AT&T's whopper $2.2 billion sale-leaseback deal involving property in Manhattan's Hudson Yards, the telecommunications giant has broadened its strategy to sell off even more of its real estate portfolio in similar sale-leaseback deals throughout the country.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_pre(u\"This is a great first apartment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinc.extra.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = datasets.imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset into spaCy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load data from the IMDB dataset.\"\"\"\n",
    "    train_data, test_data = imdb\n",
    "    \n",
    "    train_texts, train_labels = zip(*train_data)\n",
    "    train_cats = [{\"SENTIMENT\": bool(y)} for y in train_labels]\n",
    "    \n",
    "    test_texts, test_labels = zip(*test_data)\n",
    "    test_cats = [{\"SENTIMENT\": bool(y)} for y in test_labels]\n",
    "    \n",
    "    return (train_texts, train_cats), (test_texts, test_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the IMDB dataset\n",
    "print(\"Loading IMDB data...\")\n",
    "(train_texts, train_cats), (test_texts, test_cats) = load_data()\n",
    "n_texts = len(train_texts) + len(test_texts)\n",
    "print(\n",
    "    \"Using {} examples ({} training, {} test)\".format(\n",
    "        n_texts, len(train_texts), len(test_texts)\n",
    "    )\n",
    ")\n",
    "train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n",
    "test_data = list(zip(test_texts, [{\"cats\": cats} for cats in test_cats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spaCy Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en')\n",
    "\n",
    "textcat = nlp.create_pipe('textcat')        \n",
    "# add label to text classifier\n",
    "textcat.add_label('SENTIMENT')\n",
    "        \n",
    "nlp.add_pipe(textcat, last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "model_path = os.path.join(os.path.pardir, 'models', 'sentiment_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if do_train:\n",
    "\n",
    "    n_iter = 20\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Training the model...\")\n",
    "        batch_sizes = spacy.util.compounding(4.0, 32.0, 1.001)\n",
    "        for i in tqdm(range(n_iter)):\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            random.shuffle(train_data)\n",
    "            batches = spacy.util.minibatch(train_data, size=batch_sizes)\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "            print('Training loss: ' + json.dumps(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_train:\n",
    "    nlp.to_disk(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not do_train:\n",
    "    nlp = spacy.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_data, model):\n",
    "    \n",
    "    X = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for text, annotations in test_data:\n",
    "        X.append(text)\n",
    "        y_true.append(annotations['cats']['SENTIMENT'])\n",
    "        \n",
    "    docs = model.pipe(X)\n",
    "    \n",
    "    for doc in docs:\n",
    "        y_pred.append(doc.cats['SENTIMENT'] > 0.5)\n",
    "   \n",
    "    print(metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(test_data[:10000], nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    text_in = input('Text:')\n",
    "    if text_in == 'exit':\n",
    "        break\n",
    "    print(json.dumps(nlp(text_in).cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
