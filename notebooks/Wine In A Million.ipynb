{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09e963a",
   "metadata": {},
   "source": [
    "# Create Wine Recommender Using NLP on AWS SageMaker\n",
    "\n",
    "Authors: __[Zephyr Headley](https://github.com/jzheadley)__ and __[John Naylor](https://jonaylor.xyz)__\n",
    "\n",
    "Blog Post: \\<Blog Post Link>\n",
    "\n",
    "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/jonaylor89/WineInAMillion/blob/main/notebooks/Wine%20In%20A%20Million.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37fde8e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Bidirectional Encoder Representations from Transformers**¬†(**BERT**) is a¬†[transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) based¬†[machine learning](https://en.wikipedia.org/wiki/Machine_learning)¬†technique for¬†[natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing)¬†(NLP) pre-training developed by¬†[Google](https://en.wikipedia.org/wiki/Google). BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. In 2019, Google announced that it had begun leveraging BERT in¬†[its search engine](https://en.wikipedia.org/wiki/Google_Search), and by late 2020 it was using BERT in almost every English-language query. A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in NLP experiments\", counting over 150 research publications analyzing and improving the model. BERT improves on previous word2vec models but not just embedding the lone words but embedding words using the context they are in to more accurately represent them. Today, BERT has been adapted, altered, and fine tuned for many different use cases and in this notebook, we'll specifically be using **Sentence-BERT**. \n",
    "\n",
    "### What is Sentence-BERT?\n",
    "\n",
    "Sentence-BERT is a modification of pretrained BERT described well in an article [here](https://www.capitalone.com/tech/machine-learning/how-to-finetune-sbert-for-question-matching/). To quote a passage from said article:\n",
    "\n",
    "> Sentence-BERT is a word embedding model.¬†[Word embedding](https://en.wikipedia.org/wiki/Word_embedding)¬†models are used to numerically represent language by transforming phrases, words, or word pieces (parts of words) into vectors. These models can be pre-trained on a large background corpus (dataset) and then later updated with a smaller corpus that is catered towards a specific domain or task. This process is known as fine-tuning.\n",
    "> \n",
    "\n",
    "Sentence-BERT works great for the task we're going to be using it for because it has been optimized for faster similarity computation on the individual sentence level.\n",
    "\n",
    "### Nearest Neighbors\n",
    "\n",
    "The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice, along with Manhattan and Cosine Similarity. Neighbors-based methods are known as¬†*non-generalizing*¬†machine learning methods, since they simply ‚Äúremember‚Äù all of its training data (possibly transformed into a fast indexing structure such as a¬†[Ball Tree](https://scikit-learn.org/stable/modules/neighbors.html#ball-tree)¬†or¬†[KD Tree](https://scikit-learn.org/stable/modules/neighbors.html#kd-tree)). This is similar to the popular KNN algorithm except that, generally, KNN usually implies classification or regression on the neighboring sample point while *just* NN is simply returning the neighboring sample points. \n",
    "\n",
    "### Overview\n",
    "\n",
    "In this notebook, we'll demonstrate how BERT can be used in tandem with Nearest Neighbors to create a recommendation engine that uses natural language as an input. To do this, we'll take advantage of a dataset of wine reviews located [here](https://www.kaggle.com/zynicide/wine-reviews) that contains 130k different reviews of various wines. We'll use BERT to take those wine reviews, convert the reviews into word embeddings (i.e. vectors) and store those embeddings in AWS S3. With the embeddings stored in S3, we will then use that as our dataset for the Nearest Neighbor algorithm which will in turn be able to accept new user input, create an embedding for it, and find the *K* closest embeddings to that user input. **In essence finding the wines that have a review most similar to the input the user provided.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install nvidia-ml-py3\n",
    "!pip install opendatasets\n",
    "\n",
    "!pip install nb_black\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import joblib\n",
    "import sagemaker\n",
    "import opendatasets as od\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import image_uris\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "\n",
    "# Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "print(f\"SageMaker SDK Version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a73b434",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 buckets and prefixes that you want to use for saving model data and where training data is located. These should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role ARN used to give SageMaker access to your data. It can be fetched using the¬†**get_execution_role**¬†method from sagemaker python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "# bucket = \"<S3_BUCKET>\"\n",
    "# prefix = \"<S3_KEY_PREFIX>\"\n",
    "# filename = \"<DATASET_FILENAME>\"\n",
    "\n",
    "bucket = \"winemag-data-wineinamillion-23452\"\n",
    "prefix = \"data/\"\n",
    "filename = \"winemag-data-130k-v2.csv\"\n",
    "\n",
    "assert bucket != \"<S3_BUCKET>\"\n",
    "assert prefix != \"<S3_KEY_PREFIX>\"\n",
    "assert filename != \"<DATASET_FILENAME>\"\n",
    "\n",
    "raw_data_location = f\"s3://{bucket}/{prefix}raw/{filename}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e415e0d",
   "metadata": {},
   "source": [
    "### Download the dataset from Kaggle (requires Kaggle account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a113142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2021/04/how-to-download-kaggle-datasets-using-jupyter-notebook/\n",
    "od.download(\"https://www.kaggle.com/zynicide/wine-reviews\")\n",
    "inputs = boto3.resource(\"s3\").Bucket(bucket).upload_file(f\"wine-reviews/{filename}\", f\"{prefix}raw/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba173e4",
   "metadata": {},
   "source": [
    "To make sure everything worked correctly, we can read the data into a Pandas dataframe directly from S3 and then use the describe method to give us a summary of our data.  It'll kinda be nonsense but lets us know it was at least able to pull the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9808b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(raw_data_location)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c0db9",
   "metadata": {},
   "source": [
    "We can use `head()` to give us a little sample of the dataset as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56bc23",
   "metadata": {},
   "source": [
    "And we can take a look at one of the descriptions to get a better idea of what they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"description\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce22f69",
   "metadata": {},
   "source": [
    "# Preprocess & Clean Data\n",
    "\n",
    "When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. Fortunately, we aren't using *too* much data so we can use the tools provided by the SageMaker Python SDK to clearn and upload the data to a default bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(desc):\n",
    "    words = stopwords.words('english')\n",
    "    lower = \" \".join([w for w in desc.lower().split() if not w in words])\n",
    "    punct = ''.join(ch for ch in lower if ch not in punctuation)\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    word_tokens = nltk.word_tokenize(punct)\n",
    "    lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "\n",
    "    word_joined = \" \".join(lemmatized_word)\n",
    "    \n",
    "    return word_joined\n",
    "    \n",
    "\n",
    "df['clean_desc'] = df[\"description\"].apply(clean_data)\n",
    "\n",
    "print(df['clean_desc'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0caed",
   "metadata": {},
   "source": [
    "After cleaning the dataset, we upload the cleaned version to s3 for later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fece19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the preprocessed dataset to S3\n",
    "df.to_csv(\"cleaned_dataset.csv\")\n",
    "clean_data_location = f\"{prefix}clean/cleaned_dataset.csv\"\n",
    "inputs = boto3.resource(\"s3\").Bucket(bucket).upload_file('dataset.csv', clean_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ee7f9",
   "metadata": {},
   "source": [
    "\n",
    "# Sentence-BERT Embeddings\n",
    "\n",
    "There is a Python library called **`sentence-transformers`** that provides an easy method to compute dense vector representations for¬†**sentences**,¬†**paragraphs**, and¬†**images**. The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. and achieve state-of-the-art performance in various task. Text is embedding in vector space such that similar text is close and can efficiently be found using cosine similarity.\n",
    "\n",
    "The sentence tranformer we'll use is \"sentence-transformers/all-MiniLM-L6-v2\" which is fairly generic but should work if accuracy isn't the number one priority. Downloading this model returns a folder of stuff which we'll need to save to s3 and bundle somehow in order to use SageMaker to host our model. \n",
    "\n",
    "Check out the full list of sentence tranformers on HuggingFace: [https://huggingface.co/sentence-transformers](https://huggingface.co/sentence-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f61495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A peak at what the embeddings for BERT look like\n",
    "\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "saved_model_dir = 'transformer'\n",
    "if not os.path.isdir(saved_model_dir):\n",
    "    os.makedirs(saved_model_dir)\n",
    "\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(saved_model_dir)\n",
    "\n",
    "embeddings = model.encode(df[\"clean_desc\"][0])\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be1db1",
   "metadata": {},
   "source": [
    "# Generate Initial Embeddings\n",
    "\n",
    "**[WARNING]** This step will cost some money üí∞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a9da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_estimator = PyTorch(\n",
    "    role = role, \n",
    "    entry_point ='embeddings_script.py',\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    instance_count=1,\n",
    "    source_dir = './src', \n",
    "    framework_version = '1.9.0',\n",
    "    py_version = 'py38',\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    output_path=f\"s3://{bucket}/model/embeddings\",\n",
    "    hyperparameters={\n",
    "        'output-data-dir': \"/opt/ml/output/data/\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates embeddings of dataset\n",
    "embeddings_estimator.fit({'train': f\"s3://{bucket}/{prefix}clean/dataset.csv\"})\n",
    "print(\"[+] finished model fitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363155c",
   "metadata": {},
   "source": [
    "### Creating and Testing Embedding Endpoint\n",
    "\n",
    "After training, we use the¬†`PyTorchModel`¬†object to build and deploy a¬†`PyTorchPredictor`. This creates a Sagemaker Endpoint - a hosted prediction service that we can use to perform inference.\n",
    "\n",
    "We have implementation of¬†`model_fn`¬†,¬†`input_fn`, ¬†`predict_fn`, and ¬†`output_fn`¬†in the `encoding_inference.py`¬†script that is required. We are going to use default implementations of  and¬†`transform_fn`¬†defined in¬†[sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers).\n",
    "\n",
    "The serializer and deserializer configure the `ContentType` field and the `Accept` field which in our case is both `application/json` . The¬†`ContentType`¬†field configures the first container, while the¬†`Accept`¬†field configures the last container. You can also specify each container'sHere we just grab the first line from the test data (you'll notice that the inference python script is very particular about the ordering of the inference request data).¬†`Accept`¬†and¬†`ContentType`¬†values using environment variables.\n",
    "\n",
    "The arguments to the deploy function allow us to set the number and type of instances that will be used for the Endpoint. These do not need to be the same as the values we used for the training job. For example, you can train a model on a set of GPU-based instances, and then deploy the Endpoint to a fleet of CPU-based instances. Here we will deploy the model to a single¬†`ml.m5.large`¬†instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "embeddings_endpoint_name = \"embeddings-model-ep-\" + timestamp_prefix\n",
    "\n",
    "embedding_predictor = embeddings_estimator.deploy(\n",
    "    instance_type='ml.m5.large',\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=embeddings_endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daacd92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding = embedding_predictor.predict(\n",
    "    {\"data\": \"sweet wine with a hint of tartness\"}\n",
    ")\n",
    "print(len(test_embedding[\"embeddings\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38521b2f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Nearest Neighbors Model \n",
    "\n",
    "As mentioned before, Neighbors-based methods are known as¬†*non-generalizing*¬†machine learning methods, since they simply ‚Äúremember‚Äù all of its training data. The API for it is still the same as any other model (i.e. we'll still call `fit` to give the model our dataset) but there isn't actually any fitting or training going on. In our case, we'll pull the embeddings from s3 and that'll be our input dataset.\n",
    "\n",
    "The full user guide for sklearn's `NearestNeighbors` class is available here: [https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html)\n",
    "\n",
    "To see the full docs for the SKLearn Estimator and Model, see here: ¬†https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c312d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_estimator = SKLearn(\n",
    "    entry_point='nn_script.py',\n",
    "    source_dir = './src', \n",
    "    train_instance_type=\"ml.m5.large\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    framework_version=\"0.23-1\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters={\n",
    "        'n_neighbors': 10, \n",
    "        'metric': 'cosine',\n",
    "    },\n",
    ")\n",
    "    \n",
    "nn_estimator.fit({'train': f\"s3://{bucket}/model/embeddings/embeddings.csv.tar.gz\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f2d51a",
   "metadata": {},
   "source": [
    "Similar to the PyTorchModel above, after training, we use the SKLearnModel¬†object to build and deploy a¬†SKLearnPredictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd654d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "nn_endpoint_name = \"nn-model-ep-\" + timestamp_prefix\n",
    "nn_predictor = nn_estimator.deploy(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=nn_endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168eb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(\n",
    "    endpoint_name=nn_endpoint_name,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "prediction = predictor.predict(\n",
    "    {\"embeddings\": test_embedding[\"embeddings\"], \"kneighbors\": 5}\n",
    ")\n",
    "print(prediction)\n",
    "# zipped = list(\n",
    "#     zip(\n",
    "#         prediction[\"recommendations\"][\"neighbors\"][0],\n",
    "#         prediction[\"recommendations\"][\"distance\"][0],\n",
    "#     )\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab303c3",
   "metadata": {},
   "source": [
    "\n",
    "# Inference Pipeline\n",
    "\n",
    "Setting up a Machine Learning pipeline can be done with the `PipelineModel`. This sets up a list of models in a single endpoint; in this example, we configure our pipeline model with the BERT embedding model and the fitted Scikit-learn Nearest Neighbors inference model. Deploying the model follows the same¬†`deploy`¬†pattern in the SDK\n",
    "\n",
    "Inference Pipeline documentation is located here: [https://sagemaker.readthedocs.io/en/stable/api/inference/pipeline.html](https://sagemaker.readthedocs.io/en/stable/api/inference/pipeline.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d42a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "endpoint_name = \"inference-pipeline-ep-\" + timestamp_prefix\n",
    "pipeline_model = PipelineModel(\n",
    "    role=role, \n",
    "    models=[\n",
    "        embeddings_estimator.create_model(), \n",
    "        nn_estimator.create_model(),\n",
    "    ],\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d257477",
   "metadata": {},
   "source": [
    "### Hosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf816964",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline = pipeline_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5910cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd2f90",
   "metadata": {},
   "source": [
    "# Test Pipeline\n",
    "\n",
    "We make our request with the payload in¬†'application/json'¬†format, since that is what our script currently supports. If other formats need to be supported, this would have to be added to the¬†output_fn()¬†method in our entry point. The prediction output in this case is trying to guess the wines with the most similar reviews to the one inputted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d7a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_payload = json.dumps({\"data\": \"sweet wine with a hint of tartness\"})\n",
    "test_response = pipeline_predictor.predict(data=test_payload)\n",
    "\n",
    "print(test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b14af",
   "metadata": {},
   "source": [
    "# Clean Up\n",
    "\n",
    "Finally, we should delete the model and endpoint before we close the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a5c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model\n",
    "embeddings_model.delete_model()\n",
    "nn_model.delete_model()\n",
    "pipeline_predictor.delete_model()\n",
    "\n",
    "# Delete endpoint and endpoint configuration\n",
    "embeddings_predictor.delete_predictor()\n",
    "nn_predictor.delete_predictor()\n",
    "pipeline_predictor.delete_predictor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
